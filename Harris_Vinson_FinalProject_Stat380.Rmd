---
title: "R Notebook"
author: "Remington Harris and Cameron Vinson"
date: "May 1st 2019"
output: html_notebook
---

## Front Matter


```{r include=FALSE}

# Clean workspace

rm(list = ls())

# Packages used

library(mosaic)
library(tidyr)
library(readr)
library(tidyverse)
library(rvest)
library(regexPipes)
library(growthrates)
library(magicfor)
library(wordcloud)

# Inputs & source data

PopURL <-
  "https://en.wikipedia.org/wiki/List_of_countries_by_past_and_future_population"

PopXPATH<-
  '//*[@id="mw-content-text"]/div/table[2]'


EmissionsURL <-
  "http://edgar.jrc.ec.europa.eu/overview.php?v=CO2ts_pc1990-2013"

EmissionsXPATH <-
  '//*[@id="layout"]/div[3]/div[2]/div/div/div/div/table'

TotalBiocapRaw <-
  read.csv(file = "Biocapacity.csv")

BiocapPerCapRaw <-
  read.csv(file = "BiocapacityperCap.csv")

# User defined Functions

```


### Our Purpose


We felt that before jumping right into our code we made it clear first how our project tackles social good.  The topic we chose to take on was carbon emissions.  We felt that this was clearly social good as it is a problem that will be a concern that transcends borders and religion, and really will affect all people.  However, many people are still resistant to accepting this reality.  Our hope in tackling this project is that we will provide visualizations and compelling evidence that maybe makes these doubter see the problem in a different light than before.  Furthermore for those that do recognize this issue, but arent sure how it can even be addressed, we hoped to piint out where the blame mostly lies, even though all countries contribute to the problem.  Without further ado, here is our final project, of which we are very proud.


## Data Wrangling


### Loading Population Data Table


```{r}
table_list <- 
  PopURL %>%
  read_html() %>%
  html_nodes(xpath = PopXPATH) %>%
  html_table(fill = TRUE)

Pop_Country <- table_list[[1]] 

head(Pop_Country)
```


### Cleaning Population Data Table


```{r}
# Remington
index <- as.character(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))

colnames(Pop_Country) <- index

Pop_Country <-
  Pop_Country %>%
  rename(
    Country = `1`,
    `1985 Pop` = `2`,
    `1990 Pop` = `4`,
    `1995 Pop` = `6`,
    `2000 Pop` = `8`,
    `2005 Pop` = `10`,
    `2010 Pop` = `12`,
    `2015 Pop` = `14`) %>%
  select(Country, `1985 Pop`, `1990 Pop`, `1995 Pop`, `2000 Pop`, `2005 Pop`, `2010 Pop`, `2015 Pop`)

head(Pop_Country)
```

At this point we needed to remove the % columns so that the data would be in a form that is useful to us.  In order to do this Remmy Renamed the Variables that would be needed and then simply selected those.  This was out first step to wrangling the data for Population.

```{r}
# Remington
Pop_Country$`1985 Pop` <-
  Pop_Country$`1985 Pop` %>%
  regexPipes::gsub(pattern = ",", replacement = "")
  
Pop_Country$`1990 Pop` <-
  Pop_Country$`1990 Pop` %>%
  regexPipes::gsub(pattern = ",", replacement = "")

Pop_Country$`1995 Pop` <-
  Pop_Country$`1995 Pop` %>%
  regexPipes::gsub(pattern = ",", replacement = "")

Pop_Country$`2000 Pop` <-
  Pop_Country$`2000 Pop` %>%
  regexPipes::gsub(pattern = ",", replacement = "")

Pop_Country$`2005 Pop` <-
  Pop_Country$`2005 Pop` %>%
  regexPipes::gsub(pattern = ",", replacement = "")

Pop_Country$`2010 Pop` <-
  Pop_Country$`2010 Pop` %>%
  regexPipes::gsub(pattern = ",", replacement = "")

Pop_Country$`2015 Pop` <-
  Pop_Country$`2015 Pop` %>%
  regexPipes::gsub(pattern = ",", replacement = "")

Pop_Country_Cleaned <-
  Pop_Country %>%
  mutate(`1985 Pop` = as.numeric(Pop_Country$`1985 Pop`),
         `1990 Pop` = as.numeric(Pop_Country$`1990 Pop`),
         `1995 Pop` = as.numeric(Pop_Country$`1995 Pop`),
         `2000 Pop` = as.numeric(Pop_Country$`2000 Pop`),
         `2005 Pop` = as.numeric(Pop_Country$`2005 Pop`),
         `2010 Pop` = as.numeric(Pop_Country$`2010 Pop`),
         `2015 Pop` = as.numeric(Pop_Country$`2015 Pop`))

head(Pop_Country_Cleaned)
```

Furthermore we now needed the population figures to be recognized as numerics, but in order to do this we needed to remove the commas from the population numbers, which were currently character variables.  Remmy removed the commas using gsub and then converted the variables to numerics.  This was the first occurence of using regex in our project, it would come back later in a big way.


### Loading Emissions Data


```{r}
# Cameron
table_list <-
  EmissionsURL %>%
  read_html() %>%
  html_nodes(xpath = EmissionsXPATH) %>%
  html_table(fill = TRUE)

CarbonRaw <-
  table_list[[1]]

head(CarbonRaw)
```


### Cleaning Emissions Data


```{r}
# Cameron
colnames(CarbonRaw) <-
  as.character(unlist(CarbonRaw[1,]))

CarbonRaw2 <-
  CarbonRaw[-1, ]

CarbonRaw3 <-
  CarbonRaw2[-1, ]

CarbonRaw4 <-
  CarbonRaw3 %>%
  #Making names more easy to wrangle
  rename(`1990` = `1990TonsCO2/cap4.267`,
         `1995` = `1995TonsCO2/cap4.120`,
         `2000` = `2000TonsCO2/cap4.146`,
         `2005` = `2005TonsCO2/cap4.513`,
         `2010` = `2010TonsCO2/cap4.780`,
         `2011` = `2011TonsCO2/cap4.870`,
         `2012` = `2012TonsCO2/cap4.894`,
         `2013` = `2013TonsCO2/cap4.936`)

head(CarbonRaw4)
```

This was some basic cleaning to just make sure that the data was tidy, but there were still further steps to make it usable for our analysis.

```{r}
# Cameron
CarbonCleaned <-
  CarbonRaw4 %>%
  # Making Emissions a dbl variable
  transmute(`Country` = `CountryWorld Total`,
         `1990EmissionsPerCap` = as.numeric(CarbonRaw4$`1990`),
         `1995EmissionsPerCap` = as.numeric(CarbonRaw4$`1995`),
         `2000EmissionsPerCap` = as.numeric(CarbonRaw4$`2000`),
         `2005EmissionsPerCap` = as.numeric(CarbonRaw4$`2005`),
         `2010EmissionsPerCap` = as.numeric(CarbonRaw4$`2010`)) 

head(CarbonCleaned)
```

All of the numbers needed to be converted to numerics in order to be utilized appropriately.

```{r}
# Cameron
CarbonCleaned$Country <-
  CarbonCleaned$Country %>%
  # Replacing country names that are in a different form in the Emissions data than the Pop Data
  regexPipes::sub("Korea, Democratic People's Republic of", "South Korea") %>%
  regexPipes::sub("Korea, Republic of", "North Korea") %>%
  regexPipes::sub("Lao People's Democratic Republic", "Laos") %>%
  regexPipes::sub("Macedonia, the former Yugoslav Republic of", "North Macedonia") %>%
  regexPipes::sub("Libyan Arab Jamahiriya", "Libya") %>%
  regexPipes::sub("Macao", "Macau") %>%
  regexPipes::sub("Moldova, Republic of", "Moldova") %>%
  regexPipes::sub("Russian Federation", "Russia") %>%
  regexPipes::sub("Congo_the Democratic Republic of the", "Democratic Republic of the Congo") %>%
  regexPipes::sub("Congo", "Republic of the Congo") %>%
  regexPipes::sub("Democratic Republic of the Republic of the Congo", "Democratic Republic of the Congo") %>%
  regexPipes::sub("Cote d'Ivoire", "Ivory Coast") %>%
  regexPipes::sub("Iran, Islamic Republic of", "Iran") %>%
  regexPipes::sub("Sao Tome and Principe", "São Tomé and Príncipe") %>%
  regexPipes::sub("Taiwan_Province of China", "Taiwan") %>%
  regexPipes::sub("Tanzania_United Republic of", "Tanzania") %>%
  regexPipes::sub("Syrian Arab Republic", "Syria") %>%
  regexPipes::sub("Viet Nam", "Vietnam") %>%
  regexPipes::sub("Virgin Islands_British", "British Virgin Islands") %>%
  regexPipes::sub("Virgin Islands_USA", "United States Virgin Islands") %>%
  regexPipes::sub("Brunei Darussalam", "Brunei")


```

This was the most major usage of regex throughout the project.  Because of naming inconsistencies between our two sources of data thus far, we needed to manually replace certain names of countries that we knew were the same, but not recognized by the computer as such.  Althought this chunk says it was done by Cameron, this was certainly a team effort as we looked through the data tables and researched what alternative names of countries were as well as going back and forth between sources to see what was still missing after each.

Additionally this was where we introduced a new package called regexPipes which allowed for the flow of one regex exporession after another and allowed us to continually manipulate the table one country after the next.  This was very hepful for using regular expressions hand in hand with the tidyverse sublanguage.


### Loading Carrying Capacity Tables


```{r}
# Remington
TotalBiocapRaw <-
  read.csv(file = "Biocapacity.csv")
```

```{r}
# Cameron
BiocapPerCapRaw <-
  read.csv(file = "BiocapacityperCap.csv")
```

Instead of modeling Population, which we tried in vain to do, we decided instead to try and explore what our emissions would look like if the Earth were at it maximum population, also known as carrying capacity.  We pivoted to this route by finding Data that gave us the total Biocapacity of countries and the Biocapacity per capita.  Dividing the toal by the per capita actually gave us what the maximum population would look like for each country.  This souns wrong but it is due to the nature of what "per capita" meant in the context of this data.

In this case the per capita wasn't actually the current population, but rather then max number of people, which is why doing the divisuion allowed us to find the carrying capacity.  This was due to the weird way the data was presented by our source, but it eventually will serve a purpose for us that you will see later.


### Cleaning Carrying Capacity Tables


```{r}
# Remington
TotalBiocapV1 <-
  TotalBiocapRaw %>%
  select(Country.Name, Total) %>%
  rename(
    Country = Country.Name,
    TotalBioCap = Total
  )
```

```{r}
# Remington
TotalBiocapV1$Country <- as.character(TotalBiocapV1$Country)
TotalBiocapV1[76, 2] <- 58661671.8495966
TotalBiocapV1[90, 1] <- "North Korea"
TotalBiocapV1[92, 1] <- "South Korea"
TotalBiocapV1[90, 2] <- 14489062.7042875
TotalBiocapV1[92, 2] <- 33985103.3058803
TotalBiocapV1[161, 2] <- 56760871.8701572
TotalBiocapV1[178, 2] <- 80340362.408074
TotalBiocapV1[184, 1] <- "Democratic Republic of Congo"
TotalBiocapV1[184, 2] <- 199469941.479419
```

Because much of the issue that came with this data, at least intially, had to do with the numbers being "pushed over" we went and manually plugged in the number that were altered.  This was rather easy to do as we stillhad the original data and could just plug in the appropriate numbers where needed.  And the countries needed not yet to be joined to the countries in the previous data, but rather to each other.  Becuase of this the country names did not need to be adjusted drastically yet.

```{r}
# Remington
TotalBiocapV2 <-
  TotalBiocapV1 %>%
  na.omit()
```

```{r}
# Cameron
BiocapPerCapV1 <- # Some countries will need to be altered manually
  BiocapPerCapRaw %>%
  select(Country.Name, Total) %>%
  rename(BiocapPerCap = Total, Country = Country.Name)

BiocapPerCapV1$Country <-
  as.character(BiocapPerCapV1$Country)
```

```{r}
# Cameron
BiocapPerCapV1[76, 2] <- 0.73073708156185
BiocapPerCapV1[90, 1] <- "North Korea"
BiocapPerCapV1[90, 2] <- 0.571141605267523
BiocapPerCapV1[92, 1] <- "South Korea"
BiocapPerCapV1[92, 2] <- 0.669104805123585
BiocapPerCapV1[161, 2] <- 1.0213896996721
BiocapPerCapV1[178, 2] <- 2.54497767995444
BiocapPerCapV1[184, 1] <- "Democratic Republic of Congo"
BiocapPerCapV1[184, 2] <- 2.53339548470155
```

This is the same adjustment as the data table for Total Biocapacity.  Some countries were "pushed over" and we addressed this in the same way.

```{r}
# Cameron
BiocapPerCapV2 <-
  BiocapPerCapV1 %>%
  na.omit()
```

```{r}
# Cameron
CarryingNumbersRaw <-
  TotalBiocapV2 %>%
  inner_join(BiocapPerCapV2, by = "Country")
```

Now we combined the two source for a calculation so that later we could find the carrying capacities

```{r}
# Cameron
CarryingCapacities <-
  CarryingNumbersRaw %>%
  mutate(CarryingCap = CarryingNumbersRaw$TotalBioCap / CarryingNumbersRaw$BiocapPerCap)

CarryingCapacitiesClean <-
  CarryingCapacities[1:187, ]
```

In addition to countries this data broke down the carrying capacities of continents and regions, we only needed the countries which is why we selected "just" the first 187 rows, the rest were not helpful to our analysis


### Joining Data


```{r}
# Remington
JoinedTable<-
  CarbonCleaned %>%
  left_join(Pop_Country_Cleaned, by = "Country")
```

Here we joined the first two data csources in order to find the total emissions.  So we would have Emissions per capita and the population for the same years.

```{r}
glimpse(JoinedTable)
```

```{r}
# Cameron
JoinedTable2 <-
  JoinedTable %>%
  # Renaming variable and making numbers true
  mutate(`1985Pop` = JoinedTable$`1985 Pop` * 1000,
         `1990Pop` = JoinedTable$`1990 Pop` * 1000,
         `1995Pop` = JoinedTable$`1995 Pop` * 1000,
         `2000Pop` = JoinedTable$`2000 Pop` * 1000,
         `2005Pop` = JoinedTable$`2005 Pop` * 1000,
         `2010Pop` = JoinedTable$`2010 Pop` * 1000,
         `2015Pop` = JoinedTable$`2015 Pop` * 1000) %>%
  select(-`1985 Pop`, -`1990 Pop`, -`1995 Pop`, -`2000 Pop`, -`2005 Pop`, -`2010 Pop`, -`2015 Pop`)

head(JoinedTable2)
```

Here we needed to adjust the population in order to make the total emissions accurate.  The population was in thousands of people so to address this we just multiplied by a thousand.  Not exactly earth shaking but an important step nonetheless.

```{r}
# Remington
# Combined Serbia and Montenegro numbers since both were separated in the original Population data table
JoinedTable2[165, 7] <- 8281000
JoinedTable2[165, 8] <- 8371000
JoinedTable2[165, 9] <- 8363000
JoinedTable2[165, 10] <- 8338000
JoinedTable2[165, 11] <- 8203000
JoinedTable2[165, 12] <- 8012000
JoinedTable2[165, 13] <- 7825000
```

Her we addressed one specific nuissance that we couyldn't earlier.  Serbia and Montenegro were listed as seperate countries in one data table but together in another.  Remmy went and calculated the number combined by hand and manually put them into the new table that we formed.  There was probably a more "programmy" way of doing things but this came to mind first and was easy enough.

```{r}
# Cameron
GlobalNumbers <-
  JoinedTable2 %>%
  mutate( # finding total emmisions since previous was per cap
    `1990TotalEmissions` = `1990EmissionsPerCap` * `1990Pop`,
    `1995TotalEmissions` = `1995EmissionsPerCap` * `1995Pop`,
    `2000TotalEmissions` = `2000EmissionsPerCap` * `2000Pop`,
    `2005TotalEmissions` = `2005EmissionsPerCap` * `2005Pop`,
    `2010TotalEmissions` = `2010EmissionsPerCap` * `2010Pop`)

head(GlobalNumbers)
```

Now for the part we have been working towards! We found the total emissions and will use this in a visualization at the end of the project.  This is the end goal in understanding how emissions have changed over time and where we may be when the earth is at capacity.

```{r}
# Remington
GlobalNumbers <-
  na.omit(GlobalNumbers)
```

This was just us finally removing the six countries that we could not get to work out with both.  This was defintely one of our hardest decisions to make but ultimately due to the lack of size and overall impact of those countries, we decided to drop them as we didn't expect them to have a significant impact in the scope of the world.

```{r}
# Remington
EmissionsTotalData <-
  GlobalNumbers %>%
  select(1, 14:18) %>%
  rename(`1990` = `1990TotalEmissions`,
         `1995` = `1995TotalEmissions`,
         `2000` = `2000TotalEmissions`,
         `2005` = `2005TotalEmissions`,
         `2010` = `2010TotalEmissions`) %>%
  mutate(CountryID = c(1:203))
```

We wanted data with just the total emissions so we made this table here, not much else to remark on.

```{r}
# Remington
EmissionsTotalNarrow <-
  EmissionsTotalData %>%
  gather(year, emissions, 2:6) %>%
  arrange(Country)

EmissionsTotalNarrow$year <- as.numeric(EmissionsTotalNarrow$year)

head(EmissionsTotalNarrow)
```

This was critical because in order to make the model later for how per capita Emission has chaged we needed the data in this narrow format.  How exactly this is used will be more clear later but essentially we wanted each country to be able to have a time variable and a emissions value associated with it.


## Analysis


### The World at Capacity


```{r}
# Cameron
NewPop<-
  GlobalNumbers[, c(1, 7:13)]

NewPop2 <-
  t(NewPop)

CountryNames <-
  NewPop2[1,]

NewPop3 <-
  as.data.frame(NewPop2)

NewPop4 <-
  NewPop3[2:8, ]
```

We wound up not using `NewPop4` like we thought we would initially but the first iteration  was important to finding the rate of change that wew werre going to use to calculate population growth.  Like we remarked on earlier this wound up not being a path we would pursue but didn't want to delete this code since it may be used later.

```{r}
# Remington
RatesofChange <-
  NewPop %>%
  mutate(Rate = (NewPop$`2015Pop` - NewPop$`1985Pop`)/30)
```

```{r}
# Remington
ModelPopTable <- 
  cbind(GlobalNumbers$Country) %>%
  as.data.frame() %>%
  rename(
    Country = V1)

ModelPopTable$Country <- as.character(ModelPopTable$Country)
```

```{r}
# Remington
GlobalInitialPop <-
  GlobalNumbers %>%
  select(Country, `1985Pop`) %>%
  rename(
    InitialPop = `1985Pop`
  )
```

```{r}
# Remington
ModelPopTable2 <-
  ModelPopTable %>%
  right_join(GlobalInitialPop, by = "Country")
```

```{r}
# Remington
RatesofChange2 <-
  RatesofChange %>%
  select(Country, Rate)

ModelPopTable3 <-
  ModelPopTable2 %>%
  right_join(RatesofChange2, by = "Country")
```

All of this wrangling wound up not helping us as it was going to be used for a model fopr population, but since we chose to model in a different mannern it wasn't used, but again we wanted to keep it in here to show what our process was.

```{r}
# Remington
CarryingCapacitiesClean2 <-
  CarryingCapacities %>%
  select(Country, CarryingCap)

CarryingCapacitiesClean2[177, 1] <- "Samoa"
CarryingCapacitiesClean2[22, 1] <- "Brunei"
CarryingCapacitiesClean2[28, 1] <- "Cape Verde"
CarryingCapacitiesClean2[179, 1] <- "Democratic Republic of the Congo"
CarryingCapacitiesClean2[36, 1] <- "Republic of the Congo"
CarryingCapacitiesClean2[81, 1] <- "Ivory Coast"
CarryingCapacitiesClean2[93, 1] <- "Laos"
CarryingCapacitiesClean2[97, 1] <- "Libya"
CarryingCapacitiesClean2[116, 1] <- "North Macedonia"
CarryingCapacitiesClean2[139, 1] <- "Russia"
CarryingCapacitiesClean2[170, 1] <- "United States"
CarryingCapacitiesClean2[175, 1] <- "Vietnam"
CarryingCapacitiesClean2[156, 1] <- "Syria"
CarryingCapacitiesClean2[153, 1] <- "Swaziland"


```

It was at this point that we needed to update some of the country names in the carrying capacity table in anticipation of joining it with some of our other data.

```{r}
# Remington
ModelPopTable4 <- 
  ModelPopTable3 %>%
  left_join(CarryingCapacitiesClean2, by = "Country")

ModelPopTable4[159, 4] <- 628615 + 8820080
```

```{r}
# Remington
sum(is.na(ModelPopTable4$CarryingCap))
```

```{r}
# Remington
NewPop4[] <- lapply(NewPop4, function(x) as.numeric(as.character(x)))
i = 1
colMax <- function(data) sapply(data, max, na.rm = TRUE)
max(NewPop4$i)
MaxPop <- colMax(NewPop4)
```

```{r}
# Remington
CountryMax <- 
  cbind(CountryNames, MaxPop) %>%
  as.data.frame()
head(CountryMax)
```

```{r}
# Remington
# no data for these points concerning carrying capacity, so using maximum documented population during time period of population data as carrying capacity
ModelPopTable4[4, 4] <- 58000
ModelPopTable4[61, 4] <- 51000
ModelPopTable4[71, 4] <- 30000
ModelPopTable4[73, 4] <- 58000
ModelPopTable4[75, 4] <- 162000
ModelPopTable4[82, 4] <- 7142000
ModelPopTable4[84, 4] <- 332000
ModelPopTable4[97, 4] <- 106000
ModelPopTable4[110, 4] <- 593000
ModelPopTable4[115, 4] <- 396000
ModelPopTable4[128, 4] <- 11000
ModelPopTable4[131, 4] <- 272000
ModelPopTable4[146, 4] <- 3822000
ModelPopTable4[151, 4] <- 52000
ModelPopTable4[153, 4] <- 7000
ModelPopTable4[154, 4] <- 109000
ModelPopTable4[156, 4] <- 195000
ModelPopTable4[160, 4] <- 93000
ModelPopTable4[176, 4] <- 23416000
ModelPopTable4[187, 4] <- 11000
ModelPopTable4[195, 4] <- 273000
ModelPopTable4[198, 4] <- 34000
ModelPopTable4[199, 4] <- 109000
ModelPopTable4[200, 4] <- 571000
```

In the two chunks above we needed to find a way to have an appropriate carrying capacity for the countries of which there was no data.  So we decided that we wanted to be conservative in our estimates so that in  best case 

```{r}
# Cameron
sum(ModelPopTable4$CarryingCap)
sum(NewPop$`2015Pop`)
```


### Building a model for Emissions Per Cap


```{r}
# Remington
EmissionsperCapData <-
  GlobalNumbers %>%
  select(1:6) %>%
  rename(`1990` = `1990EmissionsPerCap`,
         `1995` = `1995EmissionsPerCap`,
         `2000` = `2000EmissionsPerCap`,
         `2005` = `2005EmissionsPerCap`,
         `2010` = `2010EmissionsPerCap`) %>%
  mutate(CountryID = c(1:203))
```

Here is where we finally get into some good analysis.  We start by selecting just the Country, and the Emissions per capita.  Then we added a variable called CountryID that was essentially what the index of the country is.  Because we knew we would be gathering we wanted to preserve this index.

```{r}
# Cameron
EmissionsNarrow <-
  EmissionsperCapData %>%
  gather(year, emissions, 2:6) %>%
  arrange(Country)

EmissionsNarrow$year <- as.numeric(EmissionsNarrow$year)

head(EmissionsNarrow)
```

Like stated above we were going to gather in order to build an appropriate model.  We need to have a time variable instead of having time be represented in the variable itself.  This is why we gathered

```{r}
# Cameron
magic_for(print, silent = TRUE)

  for(i in 1:203) {

CountryMods <-
  lm(emissions ~ year, data = EmissionsNarrow %>%
     filter(CountryID == i))

print(CountryMods[1])
  }

OurNewData <-
  magic_result_as_dataframe()

OurNewData$`CountryMods[1]` <-
  as.character(OurNewData$`CountryMods[1]`)

head(OurNewData)
```

Now for the very fun part.  We wrote a loop and extracted the intercept and slope for each country by using a new package we found called magic_for.  This made it much easier to extract these values however as you will see this was far from a simple thing.

```{r}
# Cameron
SplitOur<-
  strsplit(x = OurNewData$`CountryMods[1]`, split = ", ") %>%
  as.data.frame() %>%
  t() %>%
  as.data.frame()

SplitOur$V1 <-
  as.character(SplitOur$V1)

SplitOur$V2 <-
  as.character(SplitOur$V2)

JustStrings <-
  cbind(SplitOur$V1, SplitOur$V2) %>%
  as.data.frame()

JustStrings$V1 <-
  JustStrings$V1 %>%
  gsub(pattern = "list\\(coefficients = c\\(`\\(Intercept\\)` = ", replacement = "")

JustStrings$V2 <-
  JustStrings$V2 %>%
  gsub(pattern = "year = ", replacement = "")

JustStrings$V2 <-
  JustStrings$V2 %>%
  gsub(pattern = "\\)\\)", replacement = "")

JustStrings <-
  JustStrings %>%
  rename(Intercept = V1, Year = V2)

JustStrings$Intercept <-
  as.numeric(JustStrings$Intercept)

JustStrings$Year <-
  as.numeric(JustStrings$Year)

OurCoeff <-
  JustStrings %>%
  cbind(CountryNames)

OurCoeff$CountryNames <-
  as.character(OurCoeff$CountryNames)

head(OurCoeff)
```

This was all of the regex used to isolate just the numbers we wanted, although the code speaks for itself there are a few things that are key to call attention to.  First we made the table above into character strings and then split it so that the numbers, although still characters, would be in seperate columns.  Then we just used a series of gsubs to isolate the number, and then converted back to numerics.  This now gave us the vaues from the moidel which we will use to find future total emissions by country.

One small issue that occurred was that some countries had a decrease in emissions over time, so once 2023 rolled around they actually had negative total emissions.  but these were very small countries and emissions totals in the scope of the world so we still felt the model did a nice job going even that far into the future.

```{r}
# Cameron
Predictions <-
  OurCoeff$Intercept + (OurCoeff$Year * 2023)

FutureEmissions <-
  Predictions * ModelPopTable4$CarryingCap

sum(GlobalNumbers$`2010TotalEmissions`)
sum(FutureEmissions)
```

Here is where the values of the model are combined with the data to give us the final numbers.  Printed are the total emissions from 2010 and our projections for total emissions in 2023 if nothing changes.


## Data Visualizations


### Similar Countries; Unsupervised Learning -- Dendrogram


```{r}
# Cameron
Similarity2010 <-
  GlobalNumbers %>%
  top_n(50, `2010TotalEmissions`) %>%
  select(`2010EmissionsPerCap`, `2010Pop`) %>%
  scale() %>%
  as.data.frame()

Top50Labels <-
  GlobalNumbers %>%
  top_n(50, `2010TotalEmissions`) %>%
  select(Country, `2010EmissionsPerCap`, `2010Pop`)
  
head(Similarity2010)
```

```{r fig.height=8, fig.width=12}
# Cameron
Distance2010 <-
  dist(Similarity2010)

Dendro2010 <-
  Distance2010 %>%
  hclust(method = "complete")

Dendro2010 %>%
  plot(labels = Top50Labels$Country)
```

This is a dendrogram of the top 50 countries in terms of total emissions in 2010.  This helps us glean some important points.  Namely that India and China are very unique, and that maybe solving our emissions problem starts with them.  Furthermore we see that the United States is very unique and doesn't play nice with many other countries.  However Kuwait and Qatar are the most dissimilar from any other country as their first meeting is each other and bove any other first meeting point.


### Heat map of Emissions


Maybe normalize each emission type and then add them together for a compound score?

```{r}
# Cameron
HeatMapData <-
  GlobalNumbers %>%
  select(-Country) %>%
  na.omit() %>%
  scale() %>%
  as.data.frame() %>%
  mutate(`1990` = `1990EmissionsPerCap` + (0.5 * `1990TotalEmissions`),
            `1995` = `1995EmissionsPerCap` + (0.5 * `1995TotalEmissions`),
            `2000` = `2000EmissionsPerCap` + (0.5 * `2000TotalEmissions`),
            `2005` = `2005EmissionsPerCap` + (0.5 * `2005TotalEmissions`),
            `2010` = `2010EmissionsPerCap` + (0.5 * `2010TotalEmissions`))

Top30Heat <-
  HeatMapData %>%
  cbind(GlobalNumbers$Country) %>%
  rename(Country = `GlobalNumbers$Country`) %>%
  top_n(n = 30, wt = `2010TotalEmissions`)

Top30HeatMatrix <-
  Top30Heat %>%
  select(`1990EmissionsPerCap`, `1995EmissionsPerCap`,`2000EmissionsPerCap`, `2005EmissionsPerCap`, `2010EmissionsPerCap`) %>%
  rename(Cap1990 = `1990EmissionsPerCap`,
         Cap1995 = `1995EmissionsPerCap`,
         Cap2000 = `2000EmissionsPerCap`,
         Cap2005 = `2005EmissionsPerCap`,
         Cap2010 = `2010EmissionsPerCap`) %>%
  as.matrix()
```

```{r}
# Cameron
EmissionsHeatmap <-
  heatmap(x = Top30HeatMatrix, Rowv = NA, Colv = NA, labRow = Top30Heat$Country)
```

This Heatmap shows us a couple things.  The first being how the top 30 countries have changed relative to each other over time in terms of Emissions per Capita.  Red being worse.  Clearly the Unitred states has gotten worse on a per capita basis to its peers.  It is important to note that the "heat" is unique to year and each year is independent of the other in terms of the rankings.


### How Emissions Grow


```{r}
# Remington
P <-
  EmissionsTotalNarrow %>%
  ggplot(aes(x = year, y = emissions, group = year)) +
  geom_boxplot() +
  scale_y_log10() 

P
```

```{r}
# Remington
Future2023 <- 
  as.data.frame(FutureEmissions) %>%
  mutate(year = 2023) %>%
  rename(emissions = FutureEmissions)
```

```{r}
# Remington
Q <-
  Future2023 %>%
  ggplot(aes(x = year, y = emissions)) +
  geom_boxplot() +
  scale_y_log10()

Q
```

```{r}
# Remington
PQ <-
  EmissionsTotalNarrow %>%
  ggplot(aes(x = year, y = emissions, group = year)) +
  geom_boxplot() +
  geom_boxplot(aes(x = year, y = emissions), , data = Future2023) +
  scale_y_log10()

PQ
```

Even though the last boxplot is weirdly thin, it still drives home the primary point of our analysis, emissions are growing and will continue to grow, unless we do something.  Even though it seems like just marginal increases the y axis is actually logarithmic which means those seemingly small increases are actually rather significant.


### Visualizing Countries Emissions with Words


```{r}
# Remington
Future2023 <-
  Future2023 %>%
  cbind(CountryNames)

```

```{r}
# Remington
longCountry<-
  Future2023 %>%
  select(CountryNames, emissions)
```

```{r}
# Remington
set.seed(1237)
wordcloud::wordcloud(words = longCountry$CountryNames, freq = longCountry$emissions)
```

This is a visualization that we think helped capture just how much of our current emissions come from just a few countries.  China, USA, India, Russia, and a few other clearly stand out from the rest and those are the countries that will have to be held in check in ordet to prevent emissions from reaching dangerous levels.


### Simple Scatterplot of Emission over time

```{r}
# Cameron
Emissions2023 <-
  Future2023 %>%
  select(emissions, CountryNames) %>%
  rename(`2023` = emissions, Country = CountryNames)

AllEmissions <-
  Emissions2023 %>%
  left_join(EmissionsTotalData, by = "Country") %>%
  select(-CountryID)

FinalNumbers <-
  c(sum(AllEmissions$`1990`),
    sum(AllEmissions$`1995`),
    sum(AllEmissions$`2000`),
    sum(AllEmissions$`2005`),
    sum(AllEmissions$`2010`),
    sum(AllEmissions$`2023`))

FinalNumbers <-
  as.data.frame(FinalNumbers)

Years <-
  c(1990, 1995, 2000, 2005, 2010, 2023)

Years <-
  as.data.frame(Years)

FinalTable <-
  cbind(Years$Years, FinalNumbers$FinalNumbers) %>%
  as.data.frame() %>%
  rename(Year = V1, EmissionsTotal = V2)
```

```{r}
FinalTable %>%
  ggplot(aes(x = Year, y = EmissionsTotal)) +
  geom_point() +
  geom_smooth(method = lm) +
  ggtitle("Emissions through the Years")
```

This is maybe a clearer picture of how worldwide emissions are trending in total.  While the boxplot showed the distributions of countries through the years, this shows how just the totals are and makes it more clear that it is clearly increasing, and at an alarming rate.